{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project Serie 1:\n",
    "# IMDB Movie Review Sentiment Classification \n",
    "# Sub-Episode 2.1: Expanding Neural Networks in Keras\n",
    "This episode focuses on transfering the model to Keras and expanding the previously used techniques (fully connected neural networks) on much larger number of features on each datapoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Reshape, Flatten, Dropout\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/Users/tieubinh03/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/tieubinh03/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = tf.keras.datasets.imdb.get_word_index(path=\"imdb_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words count: 88584\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(word_dict)\n",
    "print(\"Total words count:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(dict(sorted(word_dict.items(), key=lambda item: item[1])))\n",
    "word_list.insert(0, str(0)) # To make the index of each word matches it position in original dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most used words: ['0', 'the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(\"10 most used words:\", word_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # Choose maximum n most used words\n",
    "required_len = 1    # But exclude the words that does not match required length\n",
    "\n",
    "def choose_word_index():\n",
    "    chosen_indexes = []\n",
    "    i = 0\n",
    "    while len(chosen_indexes) < max_features:\n",
    "        if len(word_list[i]) >= required_len:\n",
    "            chosen_indexes.append(i)\n",
    "        i += 1\n",
    "    return chosen_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words picked: 20000\n"
     ]
    }
   ],
   "source": [
    "accounted_word_indexes = choose_word_index()\n",
    "nof_features = len(accounted_word_indexes)\n",
    "print(\"Number of words picked: \" + str(nof_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input(initial_input):\n",
    "    new_input = []\n",
    "    for word_index in accounted_word_indexes:\n",
    "        if word_index in initial_input:\n",
    "            new_input.append(1)\n",
    "        else:\n",
    "            new_input.append(0)\n",
    "    return new_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set took approximately 25 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING SET:\n",
    "start_time = time.time()\n",
    "\n",
    "new_train_len = len(x_train)\n",
    "new_x_train      = np.zeros((new_train_len, nof_features))\n",
    "new_y_train      = np.zeros((new_train_len, 1))\n",
    "\n",
    "for i in range(new_train_len):\n",
    "    new_x_train[i] = construct_input(x_train[i])\n",
    "    new_y_train[i] =                 y_train[i]\n",
    "    \n",
    "print(\"Processing training set took approximately\", round((time.time() - start_time)/60), \"minutes.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing testing set took approximately 24 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TESTING SET:\n",
    "start_time = time.time()\n",
    "\n",
    "new_test_len = len(x_test)\n",
    "new_x_test      = np.zeros((new_test_len, nof_features))\n",
    "new_y_test      = np.zeros((new_test_len, 1))\n",
    "\n",
    "for i in range(new_test_len):\n",
    "    new_x_test[i] = construct_input(x_test[i])\n",
    "    new_y_test[i] =                 y_test[i]\n",
    "    \n",
    "print(\"Processing testing set took approximately\", round((time.time() - start_time)/60), \"minutes.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Machine Learning Model:\n",
    "The model is also expanded in the number of hidden layers and units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_NN():\n",
    "    X_input = Input(shape=(nof_features))\n",
    "    \n",
    "    X = Dense(500, activation='relu', \n",
    "                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                    bias_regularizer=regularizers.l2(1e-4),\n",
    "                    activity_regularizer=regularizers.l2(1e-5)\n",
    "                    )(X_input)\n",
    "    \n",
    "    X = Dense(250, activation='relu', \n",
    "                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                    bias_regularizer=regularizers.l2(1e-4),\n",
    "                    activity_regularizer=regularizers.l2(1e-5)\n",
    "                    )(X)\n",
    "    \n",
    "    X = Dense(125, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                    bias_regularizer=regularizers.l2(1e-4),\n",
    "                    activity_regularizer=regularizers.l2(1e-5)\n",
    "                    )(X)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', \n",
    "                    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), \n",
    "                    bias_regularizer=regularizers.l2(1e-4),\n",
    "                    activity_regularizer=regularizers.l2(1e-5)\n",
    "                    )(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 500)               10000500  \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 125)               31375     \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 1)                 126       \n",
      "=================================================================\n",
      "Total params: 10,157,251\n",
      "Trainable params: 10,157,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_NN = model_NN()\n",
    "model_NN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer for the model\n",
    "opt_1 = Adam(lr=0.05, decay=1e-6)\n",
    "\n",
    "model_NN.compile(optimizer=opt_1, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", threshold=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 12s 398ms/step - loss: 36.8182 - binary_accuracy: 0.5193\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 9s 349ms/step - loss: 10.2172 - binary_accuracy: 0.8067\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 9s 342ms/step - loss: 5.1671 - binary_accuracy: 0.9367\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 9s 344ms/step - loss: 3.3625 - binary_accuracy: 0.9478\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 9s 353ms/step - loss: 2.7739 - binary_accuracy: 0.9384\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 9s 371ms/step - loss: 2.3077 - binary_accuracy: 0.9511\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 9s 370ms/step - loss: 2.0912 - binary_accuracy: 0.9433\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 11s 434ms/step - loss: 1.8806 - binary_accuracy: 0.9523\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 9s 349ms/step - loss: 2.2256 - binary_accuracy: 0.8752\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 9s 376ms/step - loss: 1.9624 - binary_accuracy: 0.9350\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 10s 391ms/step - loss: 1.7057 - binary_accuracy: 0.9513\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 12s 479ms/step - loss: 1.5638 - binary_accuracy: 0.9546\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 11s 450ms/step - loss: 1.5529 - binary_accuracy: 0.9520\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 9s 369ms/step - loss: 1.6798 - binary_accuracy: 0.9218\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 9s 360ms/step - loss: 1.4655 - binary_accuracy: 0.9547\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 9s 352ms/step - loss: 1.3746 - binary_accuracy: 0.9502\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 10s 380ms/step - loss: 1.6426 - binary_accuracy: 0.9166\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 9s 372ms/step - loss: 1.5991 - binary_accuracy: 0.9510\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 9s 355ms/step - loss: 1.5518 - binary_accuracy: 0.9446\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 9s 355ms/step - loss: 1.4361 - binary_accuracy: 0.9545\n",
      "Training model took approximately 3 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model_NN.fit(new_x_train, np.array(y_train).reshape(25000, 1), epochs=20, batch_size=1000)\n",
    "\n",
    "print(\"Training model took approximately\", round((time.time() - start_time)/60), \"minutes.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Testing model:\n",
    "The model was chosen as best performance after a few tries so there is no need to split-up training set and validate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 20s 25ms/step - loss: 1.5770 - binary_accuracy: 0.8591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.577048420906067, 0.8591200113296509]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NN.evaluate(new_x_test, np.array(y_test).reshape(25000, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Summary:\n",
    "\n",
    "|-|Loss|Accuracy|Sample size|\n",
    "|-|-|-|-|\n",
    "|Training|1.43|95%|25,000|\n",
    "|Testing |1.58|86%|25,000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Thank you:\n",
    "Thank you for viewing my project. See you in the next episode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
