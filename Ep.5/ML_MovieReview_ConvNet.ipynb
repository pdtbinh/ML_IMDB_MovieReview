{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project Serie 1:\n",
    "# IMDB Movie Review Sentiment Classification \n",
    "# Episode 5: Convolutional Neural Network\n",
    "This episode focuses on fitting and testing data with the Convolutional Neural Network. Word embedding technique is still being used.\n",
    "\n",
    "## I. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Reshape, Flatten, Dropout, GRU\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Softmax, Conv1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/Users/tieubinh03/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/tieubinh03/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = tf.keras.datasets.imdb.get_word_index(path=\"imdb_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words count: 88584\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(word_dict)\n",
    "print(\"Total words count:\", vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_cmt_len = 2000\n",
    "max_index = 25000\n",
    "\n",
    "def padding(initial_x):\n",
    "    output = np.zeros((chosen_cmt_len))\n",
    "    for i in range(chosen_cmt_len):\n",
    "        if i < len(initial_x) and initial_x[i] < max_index:\n",
    "            output[i] = initial_x[i]\n",
    "        else:\n",
    "            output[i] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded = np.zeros((len(x_train), chosen_cmt_len))\n",
    "for i in range(len(x_train)):\n",
    "    x_train_padded[i] = padding(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_padded = np.zeros((len(x_test), chosen_cmt_len))\n",
    "for i in range(len(x_test)):\n",
    "    x_test_padded[i] = padding(x_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Machine Learning Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_s = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model:\n",
    "def model():\n",
    "    \n",
    "    # Retrieving inputs\n",
    "    X_input = Input(shape=(chosen_cmt_len,))\n",
    "    \n",
    "    # Embedding meanings\n",
    "    embedding = Embedding(max_index, e_s)(X_input)\n",
    "    \n",
    "    drop = Dropout(0.9)(embedding)\n",
    "    \n",
    "    conv_1 = Conv1D(filters=1, kernel_size=5, strides=1, activation='tanh', padding='causal')(drop)\n",
    "    \n",
    "    drop = Dropout(0.9)(conv_1)\n",
    "    \n",
    "    flatten = Flatten()(drop)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid',\n",
    "                   kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                   bias_regularizer=regularizers.l2(1e-4),\n",
    "                   activity_regularizer=regularizers.l2(1e-5)\n",
    "                  )(flatten)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 2000, 20)          500000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2000, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 2000, 1)           101       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2000, 1)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2001      \n",
      "=================================================================\n",
      "Total params: 502,102\n",
      "Trainable params: 502,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer for the model\n",
    "learning_rate = 5e-3\n",
    "opt = Adam(lr=learning_rate, decay=1e-5)\n",
    "model.compile(optimizer=opt, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", threshold=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 758ms/step - loss: 0.7088 - binary_accuracy: 0.4986\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.6936 - binary_accuracy: 0.5176\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 801ms/step - loss: 0.6968 - binary_accuracy: 0.5058\n",
      "Testing data:\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.6927 - binary_accuracy: 0.5280\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Fitting data:\n",
      "25/25 [==============================] - 19s 754ms/step - loss: 0.6917 - binary_accuracy: 0.5269\n",
      "Testing data:\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.6824 - binary_accuracy: 0.7065\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "Fitting data:\n",
      "25/25 [==============================] - 19s 763ms/step - loss: 0.6509 - binary_accuracy: 0.6144\n",
      "Testing data:\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.5462 - binary_accuracy: 0.7737\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 778ms/step - loss: 0.5459 - binary_accuracy: 0.7244\n",
      "Testing data:\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.4200 - binary_accuracy: 0.8412\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 769ms/step - loss: 0.4937 - binary_accuracy: 0.7660\n",
      "Testing data:\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.3943 - binary_accuracy: 0.8682\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 777ms/step - loss: 0.4633 - binary_accuracy: 0.7852\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.3659 - binary_accuracy: 0.8757\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "Fitting data:\n",
      "25/25 [==============================] - 19s 751ms/step - loss: 0.4385 - binary_accuracy: 0.7990\n",
      "Testing data:\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.3564 - binary_accuracy: 0.8793\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "Fitting data:\n",
      "25/25 [==============================] - 19s 756ms/step - loss: 0.4319 - binary_accuracy: 0.8054\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3515 - binary_accuracy: 0.8817\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 790ms/step - loss: 0.4141 - binary_accuracy: 0.8121\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3388 - binary_accuracy: 0.8839\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 776ms/step - loss: 0.4070 - binary_accuracy: 0.8187\n",
      "Testing data:\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.3416 - binary_accuracy: 0.8765\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 764ms/step - loss: 0.3945 - binary_accuracy: 0.8264\n",
      "Testing data:\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.3354 - binary_accuracy: 0.8843\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "Fitting data:\n",
      "25/25 [==============================] - 19s 776ms/step - loss: 0.3940 - binary_accuracy: 0.8236\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3347 - binary_accuracy: 0.8839\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 764ms/step - loss: 0.3965 - binary_accuracy: 0.8243\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3335 - binary_accuracy: 0.8854\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "Fitting data:\n",
      "25/25 [==============================] - 19s 777ms/step - loss: 0.3876 - binary_accuracy: 0.8291\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.3335 - binary_accuracy: 0.8856\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 775ms/step - loss: 0.3956 - binary_accuracy: 0.8228\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.3333 - binary_accuracy: 0.8858\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 778ms/step - loss: 0.3934 - binary_accuracy: 0.8272\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3332 - binary_accuracy: 0.8858\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 785ms/step - loss: 0.3905 - binary_accuracy: 0.8281\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3332 - binary_accuracy: 0.8858\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "Fitting data:\n",
      "25/25 [==============================] - 19s 778ms/step - loss: 0.3949 - binary_accuracy: 0.8246\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3332 - binary_accuracy: 0.8860\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "Fitting data:\n",
      "25/25 [==============================] - 20s 790ms/step - loss: 0.3923 - binary_accuracy: 0.8251\n",
      "Testing data:\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3332 - binary_accuracy: 0.8859\n",
      "\n",
      "\n",
      "Optimal testing accuracy is: 88.60%\n"
     ]
    }
   ],
   "source": [
    "# Storing histories\n",
    "histories = []\n",
    "testings  = []\n",
    "\n",
    "# Track testing accuracy\n",
    "prev_acc = 0\n",
    "curr_acc = 0.01\n",
    "\n",
    "# Max testing accuracy\n",
    "max_acc = 0\n",
    "\n",
    "# Fitting and evaluating the model after epochs\n",
    "epoch = 1\n",
    "\n",
    "# Keep training as long as testing accuracy on testing set is still increasing\n",
    "while epoch < 21:\n",
    "    # Fitting\n",
    "    print(\"Epoch:\", epoch)\n",
    "    print(\"Fitting data:\")\n",
    "    history = model.fit(x = x_train_padded, y = np.array(y_train).reshape(25000, 1), epochs=1, batch_size=1000)\n",
    "    \n",
    "    # Evaluating\n",
    "    print(\"Testing data:\")\n",
    "    testing = model.evaluate(x_test_padded, np.array(y_test).reshape(25000, 1))\n",
    "    \n",
    "    # Assigning max accuracy\n",
    "    if testing[1] > max_acc:\n",
    "        max_acc = testing[1]\n",
    "        \n",
    "    # Assigning test accuracy\n",
    "    prev_acc = curr_acc\n",
    "    curr_acc = testing[1]\n",
    "        \n",
    "    # Adjust learning rate\n",
    "    if prev_acc > curr_acc:\n",
    "        learning_rate /= 10\n",
    "        opt = Adam(lr=learning_rate, decay=1e-5)\n",
    "        model.compile(optimizer=opt, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", threshold=0.5)])\n",
    "    \n",
    "    # Storing\n",
    "    histories.append(history)\n",
    "    testings.append(testing)\n",
    "    \n",
    "    epoch += 1\n",
    "    print('\\n')\n",
    "\n",
    "print(\"Optimal testing accuracy is: {:.2f}%\".format(max_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2474 - binary_accuracy: 0.9342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24780705571174622, 0.9339600205421448]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train_padded, np.array(y_train).reshape(25000, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Summary:\n",
    "Convolutional Network performed the best on a similar number of parameters to other models' (approx 500,000 parameters) and very little training time.\n",
    "\n",
    "|-|Loss|Accuracy|Sample size|\n",
    "|-|-|-|-|\n",
    "|Training|1.25|93.4%|25,000|\n",
    "|Testing |0.33|88.6%|25,000|\n",
    "\n",
    "The data in the given sample seems to have simple general structures as shallower neural networks work much better than deep ones (this was a sub-test not being shown in this report)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Thank you:\n",
    "Thank you for viewing my project. This is the final episode of this serie. One of the important things I learnt from this serie was that deeper neural networks do not always mean better performances. If the depth and complexity of the network is increased exceeding some specific threshold, it will hurt performance. The right model should be chosen based on analysis of the complexity and structure of the given data. At the beginning of the project, I was trying to create a model that can generalize well for data from various distributions, but that model-centric approach seems to be not as efficient as I thought, data-centric might still be a more useful approach in most cases, as stated by Professor Andrew Ng.\n",
    "\n",
    "That's it for this serie. Hope to see you in the next serie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
