{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project Serie 1:\n",
    "# IMDB Movie Review Sentiment Classification \n",
    "# Episode 4: Attention Mechanism\n",
    "This episode focuses on fitting and testing data with Recurrent Neural Network (LSTM variant) combined with the simplified version of Attention Mechanism. Word embedding matrix is still being used.\n",
    "\n",
    "## I. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Reshape, Flatten, Dropout, GRU\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Softmax, SimpleRNN\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/Users/tieubinh03/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/tieubinh03/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = tf.keras.datasets.imdb.get_word_index(path=\"imdb_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words count: 88584\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(word_dict)\n",
    "print(\"Total words count:\", vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_cmt_len = 200\n",
    "max_index = 20000\n",
    "\n",
    "def padding(initial_x):\n",
    "    output = np.zeros((chosen_cmt_len))\n",
    "    for i in range(chosen_cmt_len):\n",
    "        if i < len(initial_x) and initial_x[i] < max_index:\n",
    "            output[i] = initial_x[i]\n",
    "        else:\n",
    "            output[i] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded = np.zeros((len(x_train), chosen_cmt_len))\n",
    "for i in range(len(x_train)):\n",
    "    x_train_padded[i] = padding(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_padded = np.zeros((len(x_test), chosen_cmt_len))\n",
    "for i in range(len(x_test)):\n",
    "    x_test_padded[i] = padding(x_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Machine Learning Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_s = 20\n",
    "e_s = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model:\n",
    "def model():\n",
    "    \n",
    "    X_input = Input(shape=(chosen_cmt_len,))\n",
    "    \n",
    "    embedding = Embedding(max_index, e_s)(X_input)\n",
    "    \n",
    "    drop = Dropout(0.95)(embedding)\n",
    "    \n",
    "    score = LSTM(chosen_cmt_len, activation='softmax')(drop)\n",
    "    \n",
    "    dot = Dot(axes=1)([embedding, score])\n",
    "    \n",
    "    drop = Dropout(0.95)(dot)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid',\n",
    "                   kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                   bias_regularizer=regularizers.l2(1e-4),\n",
    "                   activity_regularizer=regularizers.l2(1e-5)\n",
    "                  )(drop)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 200, 20)      400000      input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 200, 20)      0           embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 200)          176800      dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dot_20 (Dot)                    (None, 20)           0           embedding_20[0][0]               \n",
      "                                                                 lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 20)           0           dot_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            21          dropout_40[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 576,821\n",
      "Trainable params: 576,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer for the model\n",
    "learning_rate = 5e-3\n",
    "opt = Adam(lr=learning_rate, decay=1e-6)\n",
    "model.compile(optimizer=opt, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", threshold=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Fitting data:\n",
      "25/25 [==============================] - 74s 3s/step - loss: 0.6926 - binary_accuracy: 0.5111\n",
      "Testing data:\n",
      "782/782 [==============================] - 48s 62ms/step - loss: 0.6895 - binary_accuracy: 0.6783\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Fitting data:\n",
      "25/25 [==============================] - 72s 3s/step - loss: 0.6857 - binary_accuracy: 0.5996\n",
      "Testing data:\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.6809 - binary_accuracy: 0.7223\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.6731 - binary_accuracy: 0.6260\n",
      "Testing data:\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.6636 - binary_accuracy: 0.7371\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.6518 - binary_accuracy: 0.6509\n",
      "Testing data:\n",
      "782/782 [==============================] - 55s 71ms/step - loss: 0.6366 - binary_accuracy: 0.7529\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.6248 - binary_accuracy: 0.6672\n",
      "Testing data:\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.6037 - binary_accuracy: 0.7698\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.5991 - binary_accuracy: 0.6850\n",
      "Testing data:\n",
      "782/782 [==============================] - 50s 64ms/step - loss: 0.5699 - binary_accuracy: 0.7968\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.5727 - binary_accuracy: 0.6979\n",
      "Testing data:\n",
      "782/782 [==============================] - 50s 64ms/step - loss: 0.5379 - binary_accuracy: 0.8086\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.5515 - binary_accuracy: 0.7103\n",
      "Testing data:\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.5101 - binary_accuracy: 0.8244\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.5313 - binary_accuracy: 0.7175\n",
      "Testing data:\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4857 - binary_accuracy: 0.8335\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "Fitting data:\n",
      "25/25 [==============================] - 74s 3s/step - loss: 0.5174 - binary_accuracy: 0.7240\n",
      "Testing data:\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4649 - binary_accuracy: 0.8394\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.5044 - binary_accuracy: 0.7284\n",
      "Testing data:\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4476 - binary_accuracy: 0.8435\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.4932 - binary_accuracy: 0.7372\n",
      "Testing data:\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4337 - binary_accuracy: 0.8446\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "Fitting data:\n",
      "25/25 [==============================] - 74s 3s/step - loss: 0.4856 - binary_accuracy: 0.7356\n",
      "Testing data:\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4213 - binary_accuracy: 0.8492\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.4767 - binary_accuracy: 0.7402\n",
      "Testing data:\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4120 - binary_accuracy: 0.8523\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.4688 - binary_accuracy: 0.7478\n",
      "Testing data:\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4028 - binary_accuracy: 0.8525\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.4647 - binary_accuracy: 0.7471\n",
      "Testing data:\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.3964 - binary_accuracy: 0.8556\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "Fitting data:\n",
      "25/25 [==============================] - 74s 3s/step - loss: 0.4568 - binary_accuracy: 0.7519\n",
      "Testing data:\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.3909 - binary_accuracy: 0.8550\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "Fitting data:\n",
      "25/25 [==============================] - 74s 3s/step - loss: 0.4566 - binary_accuracy: 0.7547\n",
      "Testing data:\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.3899 - binary_accuracy: 0.8555\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.4522 - binary_accuracy: 0.7526\n",
      "Testing data:\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.3891 - binary_accuracy: 0.8559\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.4512 - binary_accuracy: 0.7507\n",
      "Testing data:\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.3884 - binary_accuracy: 0.8564\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "Fitting data:\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.4521 - binary_accuracy: 0.7518\n",
      "Testing data:\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.3878 - binary_accuracy: 0.8560\n",
      "\n",
      "\n",
      "Optimal testing accuracy is: 85.64%\n"
     ]
    }
   ],
   "source": [
    "# Storing histories\n",
    "histories = []\n",
    "testings  = []\n",
    "\n",
    "# Track testing accuracy\n",
    "prev_acc = 0\n",
    "curr_acc = 0.01\n",
    "\n",
    "# Max testing accuracy\n",
    "max_acc = 0\n",
    "\n",
    "# Fitting and evaluating the model after epochs\n",
    "epoch = 1\n",
    "\n",
    "# Keep training as long as testing accuracy on testing set is still increasing\n",
    "while epoch < 21 or prev_acc < curr_acc:\n",
    "    # Fitting\n",
    "    print(\"Epoch:\", epoch)\n",
    "    print(\"Fitting data:\")\n",
    "    history = model.fit(x = x_train_padded, y = np.array(y_train).reshape(25000, 1), epochs=1, batch_size=1000)\n",
    "    \n",
    "    # Evaluating\n",
    "    print(\"Testing data:\")\n",
    "    testing = model.evaluate(x_test_padded, np.array(y_test).reshape(25000, 1))\n",
    "    \n",
    "    # Assigning max accuracy\n",
    "    if testing[1] > max_acc:\n",
    "        max_acc = testing[1]\n",
    "        \n",
    "    # Assigning test accuracy\n",
    "    prev_acc = curr_acc\n",
    "    curr_acc = testing[1]\n",
    "        \n",
    "    # Adjust learning rate\n",
    "    if prev_acc > curr_acc:\n",
    "        learning_rate /= 10\n",
    "        opt = Adam(lr=learning_rate, decay=1e-5)\n",
    "        model.compile(optimizer=opt, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", threshold=0.5)])\n",
    "    \n",
    "    # Storing\n",
    "    histories.append(history)\n",
    "    testings.append(testing)\n",
    "    \n",
    "    epoch += 1\n",
    "    print('\\n')\n",
    "\n",
    "print(\"Optimal testing accuracy is: {:.2f}%\".format(max_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Summary:\n",
    "LSTM with simplified Attention mechanism took longer to train but has lower testing score. My prediction is that the data given was quite structually simple, hence, the structure of LSTM or other kinds of Recurrent Neural Network over-complicated the relationship between features and labels. The model was also tested with GRU, and SimpleRNN (not shown), but none of those achieved higher testing score.\n",
    "\n",
    "|-|Loss|Accuracy|Sample size|\n",
    "|-|-|-|-|\n",
    "|Training (with Dropout)|0.45|75.2%|25,000|\n",
    "|Testing |0.33|85.6%|25,000|\n",
    "\n",
    "Training score is lower than testing score as the Dropout was not turned off. Next project episode will turn off Dropout on final training evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Thank you:\n",
    "Thank you for viewing my project. See you in the next episode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
